from app.graph.state import ContentState
from app.services.llm_service import generate_text


def build_prompt(
    question: str,
    retrieved_chunks: list[str] | None
) -> str:
    """
    Build the final prompt depending on whether RAG is active or not.
    """

    # ğŸŸ¢ Cas 1 â€” Pas de document â†’ LLM normal
    if retrieved_chunks is None:
        return question

    # ğŸ”µ Cas 2 â€” Document fourni MAIS info absente
    if not retrieved_chunks:
        return f"""
Le document fourni ne contient pas d'information permettant
de rÃ©pondre Ã  la question suivante :

QUESTION:
{question}

RÃ©ponds clairement que l'information n'est pas disponible
dans le document.
"""

    # ğŸ”µ Cas 3 â€” RAG actif avec contenu
    context = "\n\n".join(f"- {chunk}" for chunk in retrieved_chunks)

    return f"""
Tu es un assistant qui rÃ©pond STRICTEMENT
en te basant sur le CONTEXTE ci-dessous.

CONTEXTE:
{context}

QUESTION:
{question}

RÃˆGLES IMPORTANTES:
- Utilise uniquement les informations du CONTEXTE.
- Si la rÃ©ponse n'est pas prÃ©sente, dis-le clairement.
- N'utilise aucune connaissance externe.
"""


def writer_node(state: ContentState) -> ContentState:
    question = state["prompt"]
    retrieved_chunks = state.get("retrieved_chunks")

    final_prompt = build_prompt(question, retrieved_chunks)

    generated = generate_text(final_prompt)

    state["generated_text"] = generated
    return state
